{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"\\n\\n\" + name + ' done in ' + str(round(time.time() - t0)) + 's \\n')\n",
    "\n",
    "print(\"\\n\\nStarting\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Importing and setting up libraries\"):\n",
    "    import os\n",
    "    import csv\n",
    "    import math\n",
    "    import numpy as np \n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import collections\n",
    "    import string\n",
    "    import re\n",
    "    from sklearn.model_selection import KFold\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.preprocessing import sequence\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    import time\n",
    "    import pickle\n",
    "    from nltk.corpus import words\n",
    "    import sys \n",
    "    from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from keras import backend\n",
    "\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    allEnglishWords = words.words()\n",
    "    allEnglishWords[:] = [x.lower() for x in allEnglishWords]\n",
    "\n",
    "    def clean(s):\n",
    "        transalator = str.maketrans(\"\",\"\",string.punctuation)\n",
    "        return s.translate(transalator)\n",
    "\n",
    "    def preprocess(text):\n",
    "        text = clean(text).lower()\n",
    "        text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Reading data\"):\n",
    "    x = []\n",
    "    y = []\n",
    "    radical = []\n",
    "    radicalOne = 0\n",
    "    with open(\"input.csv\",'r', encoding=\"utf8\") as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        p = 0\n",
    "        for row in reader:\n",
    "            if(len(row) == 2):\n",
    "                s = row[0]\n",
    "                x.append(preprocess(s))\n",
    "                if(row[1] != '0'):\n",
    "                    radicalOne += 1 \n",
    "                radical.append(0 if row[1] == '0' else 1)\n",
    "            p = p + 1   \n",
    "    csvFile.close           \n",
    "\n",
    "    print(\"Size of x:\",len(x),\" Size of y:\",len(radical),\" Positive : \",radicalOne)\n",
    "    X = []\n",
    "    for t in x:\n",
    "        t = re.sub(r'[^\\w\\s]',' ',t)\n",
    "        t = ' '.join([word for word in t.split() if word != \" \"])\n",
    "        t = t.lower()\n",
    "        t = ' '.join([word for word in t.split() if word not in cachedStopWords])\n",
    "        X.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"making Tokeniser\"):\n",
    "    print(\"Type of X:\",type(X)) \n",
    "    Features = X\n",
    "    Radical = radical\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "    iteration = 0\n",
    "    gRadicalAccu = 0\n",
    "    gPrecision = [0,0]\n",
    "    gRecall = [0,0]\n",
    "    gFScore = [0,0]\n",
    "    relevantIter = 0\n",
    "\n",
    "    vocabSize = len(allEnglishWords)\n",
    "    tokenizer = Tokenizer(num_words= vocabSize)\n",
    "    tokenised = tokenizer.fit_on_texts(allEnglishWords)\n",
    "\n",
    "\n",
    "    gPositivePredRadical = 0\n",
    "\n",
    "    with timer(\"trying to clear GPU memory initially \"):\n",
    "        backend.clear_session()\n",
    "\n",
    "        for i in range(20):\n",
    "            gc.collect()\n",
    "\n",
    "    with timer(\"Making Label vector\"):\n",
    "        Y = np.zeros([len(Radical),3],dtype = int)        \n",
    "        for x in range(0, len(Radical)):\n",
    "            Y[x,Radical[x]] = 1\n",
    "        for x in Y:\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Cross Validation\"):\n",
    "\n",
    "    with timer(\"Making Embedding_index dict\"):\n",
    "        embeddings_index = dict()\n",
    "        f = open('glove.twitter.27B/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    with timer(\"Making Embedding Matrix\"):\n",
    "        embedding_matrix = np.zeros((vocabSize, 100))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > vocabSize - 1:\n",
    "                break\n",
    "            else:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[index] = embedding_vector\n",
    "        \n",
    "    for train_index, test_index in kf.split(Features):\n",
    "        with timer(\"Making nueral network for iteration : \" + str(iteration + 1)):\n",
    "            iteration += 1\n",
    "            print(\"\\n\\n\\n\\nMaking nueral Network for iteration:\",iteration)\n",
    "\n",
    "            #Making Training and Testing Data\n",
    "            X_Train = [Features[x] for x in train_index]\n",
    "            X_Test = [Features[x] for x in test_index]\n",
    "            radicalTrain = [Y[x] for x in train_index]\n",
    "            radicalTest = [Y[x] for x in test_index]\n",
    "            radiclTest1 = [Radical[x] for x in test_index]\n",
    "\n",
    "            tokenisedTrain = tokenizer.texts_to_sequences(X_Train)\n",
    "            tokenisedTest = tokenizer.texts_to_sequences(X_Test)\n",
    "\n",
    "            max_review_length = 180\n",
    "            X_Train = sequence.pad_sequences(tokenisedTrain, maxlen=max_review_length,padding='post')\n",
    "            X_Test = sequence.pad_sequences(tokenisedTest, maxlen=max_review_length,padding='post')\n",
    "\n",
    "            #Radical\n",
    "            radicalModel = Sequential()\n",
    "            radicalModel.add(Embedding(vocabSize, 100, input_length=max_review_length,weights=[embedding_matrix]))\n",
    "            radicalModel.add(Dropout(0.2))\n",
    "            radicalModel.add(Conv1D(64, 5, activation='relu'))\n",
    "            radicalModel.add(MaxPooling1D(pool_size=4))\n",
    "            radicalModel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "            radicalModel.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "            radicalModel.add(LSTM(16, dropout=0.2, recurrent_dropout=0.2))\n",
    "            radicalModel.add(Dense(3, activation='sigmoid'))\n",
    "            radicalModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            radicalModel.fit(X_Train,radicalTrain,epochs=15, batch_size=100)\n",
    "            radicalScore = radicalModel.evaluate(X_Test,radicalTest,verbose = 100)\n",
    "            accuRadicalLstm = radicalScore[1]\n",
    "            print(\"\\nRadical Training Done for Iteration\",iteration)\n",
    "            positiveRadical = [x for x in radicalTest if x[0] == 0)\n",
    "            predictRadical = radicalModel.predict_classes(X_Test, verbose = 1)\n",
    "            positivePredRadical = [x for x in predictRadical if x > 0]\n",
    "            prec, recall, fscore, support = precision_recall_fscore_support(radicalTest, predictRadical)\n",
    "            print(\"Number of positive Examples : \",len(positiveRadical),  \"\\nratio : \", (len(positiveRadical) / len(radicalTest)), \"\\nPositive Predicted : \", len(positivePredRadical), \"\\naccuracy : \", accuRadicalLstm, \"\\nwrongness : \", 1 - accuRadicalLstm,\"\\n\\nPrecision : \",prec,\"\\nRecall : \", recall, \"\\nf1Score : \", fscore, \"\\nsupport : \", support )\n",
    "\n",
    "            if(accuRadicalLstm > 0.8):\n",
    "                gRadicalAccu += accuRadicalLstm\n",
    "                gPositivePredRadical += len(positivePredRadical)\n",
    "                gPrecision[0] += prec[0]\n",
    "                gPrecision[1] += prec[1]\n",
    "                gRecall[0] += recall[0]\n",
    "                gRecall[1] += recall[1]\n",
    "                gFScore[0] += fscore[0]\n",
    "                gFScore[1] += fscore[1]\n",
    "                relevantIter += 1\n",
    "\n",
    "            with timer(\"trying to clear GPU memory\"):\n",
    "                del radicalModel\n",
    "\n",
    "                backend.clear_session()\n",
    "\n",
    "                for i in range(20):\n",
    "                    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"final Output\"):\n",
    "\n",
    "    gRadicalAccu /= relevantIter\n",
    "    gPrecision = [x / relevantIter for x in gPrecision]\n",
    "    gRecall = [x / relevantIter for x in gRecall]\n",
    "    gFScore = [x / relevantIter for x in gFScore]\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n\\nOverall AccuracyScores for LSTM :\",\"\\nRadical: \",gRadicalAccu)\n",
    "    print(\"Precision : \", gPrecision)\n",
    "    print(\"Recal : \", gRecall)\n",
    "    print(\"FScore : \", gFScore)\n",
    "    print(\"Positive Predicitions in total : \")\n",
    "    print(\"Radical : \", gPositivePredRadical)\n",
    "    print(\"Positive Predicitions in average : \")\n",
    "    print(\"Radical : \", gPositivePredRadical / 10)   \n",
    "    print(\"Relevant Iterations : \", relevantIter) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
