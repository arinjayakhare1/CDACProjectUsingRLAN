{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import time\n",
    "import pickle\n",
    "from nltk.corpus import words\n",
    "import sys \n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "allEnglishWords = words.words()\n",
    "allEnglishWords[:] = [x.lower() for x in allEnglishWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"\\n\\n\" + name + ' done in ' + str(round(time.time() - t0)) + 's \\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean(s):\n",
    "\ttransalator = str.maketrans(\"\",\"\",string.punctuation)\n",
    "\treturn s.translate(transalator)\n",
    "\n",
    "def preprocess(text):\n",
    "\ttext = clean(text).lower()\n",
    "\ttext = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x: 100000  Size of y: 100000  Positive :  50000\n",
      "\n",
      "\n",
      "Reading data done in 8s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Reading data\"):\n",
    "\tx = []\n",
    "\ty = []\n",
    "\tradical = []\n",
    "\tradicalOne = 0\n",
    "\twith open(\"input.csv\",'r', encoding=\"utf8\") as csvFile:\n",
    "\t\treader = csv.reader(csvFile)\n",
    "\t\tp = 0\n",
    "\t\tfor row in reader:\n",
    "\t\t\tif(len(row) == 2):\n",
    "\t\t\t\ts = row[0]\n",
    "\t\t\t\tx.append(preprocess(s))\n",
    "\t\t\t\tif(row[1] != '0'):\n",
    "\t\t\t\t\tradicalOne += 1\t\n",
    "\t\t\t\tradical.append(0 if row[1] == '0' else 1)\n",
    "\t\t\tp = p + 1\t\n",
    "\tcsvFile.close\t\t\t\n",
    "\n",
    "\tprint(\"Size of x:\",len(x),\" Size of y:\",len(radical),\" Positive : \",radicalOne)\n",
    "\tX = []\n",
    "\tfor t in x:\n",
    "\t\tt = re.sub(r'[^\\w\\s]',' ',t)\n",
    "\t\tt = ' '.join([word for word in t.split() if word != \" \"])\n",
    "\t\tt = t.lower()\n",
    "\t\tt = ' '.join([word for word in t.split() if word not in cachedStopWords])\n",
    "\t\tX.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X: <class 'list'>\n",
      "\n",
      "\n",
      "making Tokeniser done in 18s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with timer(\"making Tokeniser\"):\n",
    "    print(\"Type of X:\",type(X))\t\n",
    "    Features = X\n",
    "    Radical = radical\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "    iteration = 0\n",
    "    gRadicalAccu = 0\n",
    "\n",
    "    gPrecision = [0,0]\n",
    "    gRecall = [0,0]\n",
    "    gFScore = [0,0]\n",
    "\n",
    "    vocabSize = len(allEnglishWords)\n",
    "    tokenizer = Tokenizer(num_words= vocabSize)\n",
    "    tokenised = tokenizer.fit_on_texts(allEnglishWords)\n",
    "\n",
    "\n",
    "    gPositivePredRadical = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-b1d426af94eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Making Embedding_index dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.twitter.27B/glove.twitter.27B.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with timer(\"Cross Validation\"):\n",
    "    \n",
    "    \n",
    "    with timer(\"Training for Iteration : \" + str(iteration + 1)):    \n",
    "        for train_index, test_index in kf.split(Features):\n",
    "            with timer(\"Making Embedding_index dict\"):\n",
    "                embeddings_index = dict()\n",
    "                f = open('glove.twitter.27B/glove.twitter.27B.100d.txt')\n",
    "                for line in f:\n",
    "                    values = line.split()\n",
    "                    word = values[0]\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "                f.close()\n",
    "                print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "            with timer(\"Making Embedding Matrix\"):\n",
    "                embedding_matrix = np.zeros((vocabSize, 100))\n",
    "                for word, index in tokenizer.word_index.items():\n",
    "                    if index > vocabSize - 1:\n",
    "                        break\n",
    "                    else:\n",
    "                        embedding_vector = embeddings_index.get(word)\n",
    "                        if embedding_vector is not None:\n",
    "                            embedding_matrix[index] = embedding_vector\n",
    "            iteration += 1\n",
    "            print(\"\\n\\n\\n\\nMaking nueral Network for iteration:\",iteration)\n",
    "\n",
    "            #Making Training and Testing Data\n",
    "            X_Train = [Features[x] for x in train_index]\n",
    "            X_Test = [Features[x] for x in test_index]\n",
    "            radicalTrain = [Radical[x] for x in train_index]\n",
    "            radicalTest = [Radical[x] for x in test_index]\n",
    "\n",
    "            tokenisedTrain = tokenizer.texts_to_sequences(X_Train)\n",
    "            tokenisedTest = tokenizer.texts_to_sequences(X_Test)\n",
    "\n",
    "            max_review_length = 180\n",
    "            X_Train = sequence.pad_sequences(tokenisedTrain, maxlen=max_review_length,padding='post')\n",
    "            X_Test = sequence.pad_sequences(tokenisedTest, maxlen=max_review_length,padding='post')\n",
    "\n",
    "            #Radical\n",
    "            radicalModel = Sequential()\n",
    "            radicalModel.add(Embedding(vocabSize, 100, input_length=max_review_length,weights=[embedding_matrix], trainable=False))\n",
    "            radicalModel.add(Dropout(0.2))\n",
    "            radicalModel.add(Conv1D(64, 5, activation='relu'))\n",
    "            radicalModel.add(MaxPooling1D(pool_size=4))\n",
    "            radicalModel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "            radicalModel.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "            radicalModel.add(LSTM(16, dropout=0.2, recurrent_dropout=0.2))\n",
    "            radicalModel.add(Dense(1, activation='sigmoid'))\n",
    "            radicalModel.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n",
    "            radicalModel.fit(X_Train,radicalTrain,epochs=10, batch_size=100)\n",
    "            radicalScore = radicalModel.evaluate(X_Test,radicalTest,verbose = 100)\n",
    "            accuRadicalLstm = radicalScore[1]\n",
    "            print(\"\\nRadical Training Done for Iteration\",iteration)\n",
    "            positiveRadical = [x for x in radicalTest if x == 1]\n",
    "            predictRadical = radicalModel.predict_classes(X_Test, verbose = 1)\n",
    "            positivePredRadical = [x for x in predictRadical if x > 0]\n",
    "            prec, recall, fscore, support = precision_recall_fscore_support(radicalTest, predictRadical)\n",
    "            print(\"Number of positive Examples : \",len(positiveRadical),  \"\\nratio : \", (len(positiveRadical) / len(radicalTest)), \"\\nPositive Predicted : \", len(positivePredRadical), \"\\naccuracy : \", accuRadicalLstm, \"\\nwrongness : \", 1 - accuRadicalLstm,\"\\n\\nPrecision : \",prec,\"\\nRecall : \", recall, \"\\nf1Score : \", fscore, \"\\nsupport : \", support )\n",
    "\n",
    "\n",
    "        gRadicalAccu += accuRadicalLstm\n",
    "        gPositivePredRadical += len(positivePredRadical)\n",
    "        \n",
    "        gPrecision[0] += prec[0]\n",
    "        gPrecision[1] += prec[1]\n",
    "        gRecall[0] += recall[0]\n",
    "        gRecall[1] += recall[1]\n",
    "        gFScore[0] += fscore[0]\n",
    "        gFScore[1] += fscore[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"final Output\"):\n",
    "    gRadicalAccu /= 10\n",
    "    gPrecision = [x / 10 for x in gPrecision]\n",
    "    gRecall = [x / 10 for x in gRecall]\n",
    "    gFScore = [x / 10 for x in gFScore]\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n\\nOverall AccuracyScores for LSTM :\",\"\\nRadical: \",gRadicalAccu)\n",
    "    print(\"Precision : \", gPrecision)\n",
    "    print(\"Recal : \", gRecall)\n",
    "    print(\"FScore : \", gFScore)\n",
    "    print(\"Positive Predicitions in total : \")\n",
    "    print(\"Radical : \", gPositivePredRadical)\n",
    "    print(\"Positive Predicitions in average : \")\n",
    "    print(\"Radical : \", gPositivePredRadical / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
